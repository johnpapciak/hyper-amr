"""Data utilities for the hyperbolic AMR detection pipeline.

This module lifts the preprocessing logic out of the exploratory notebook
(`amr_eda.ipynb`) into reusable functions:
- Download genomes (helper) and run AMRFinder on multi-FASTA genomes.
- Parse AMRFinder TSVs into canonical columns.
- Build positive/negative contig labels and AMR class lists.
- Attach sequences, hash them into fixed-length vectors, and split data.
"""

from __future__ import annotations

import json
import subprocess
import sys
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from pathlib import Path
from typing import List, Sequence
from urllib.parse import urlparse
from urllib.request import urlretrieve

import numpy as np
import pandas as pd


@dataclass
class AmrArtifacts:
    """Paths to data artifacts generated by the pipeline."""

    labels_path: Path
    classes_path: Path


@dataclass
class HashingConfig:
    k: int = 5
    buckets: int = 4096
    stride: int = 1
    max_len: int | None = None


# ----------------------------- I/O helpers -----------------------------

def download_genomes(urls: Sequence[str], output_dir: Path) -> List[Path]:
    """Download genomes from a list of URLs.

    This helper keeps the CLI self-contained; callers can point to any HTTP(S)
    endpoint hosting FASTA files. Files are saved under ``output_dir`` using the
    basename from the URL.
    """

    output_dir.mkdir(parents=True, exist_ok=True)
    paths: List[Path] = []
    for url in urls:
        target = output_dir / Path(urlparse(url).path).name
        target.parent.mkdir(parents=True, exist_ok=True)
        print(f"[download] {url} -> {target}")
        urlretrieve(url, target)
        paths.append(target)
    return paths


def run_amrfinder_on_fasta(
    fasta_path: Path,
    out_dir: Path,
    threads: int = 4,
    db: Path | None = None,
) -> Path:
    """Run AMRFinderPlus on a nucleotide FASTA and return the TSV path."""

    out_dir.mkdir(parents=True, exist_ok=True)
    fasta_path = Path(fasta_path)
    out_tsv = out_dir / f"{fasta_path.stem}.amrfinder.tsv"

    cmd = [
        "amrfinder",
        "--nucleotide",
        str(fasta_path),
        "--threads",
        str(threads),
        "--output",
        str(out_tsv),
        "--plus",
    ]
    if db is not None:
        cmd += ["--database", str(db)]

    print("[amrfinder]", " ".join(cmd))
    res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if res.returncode != 0:
        print("STDOUT:\n", res.stdout)
        print("STDERR:\n", res.stderr, file=sys.stderr)
        raise RuntimeError(f"AMRFinder failed on {fasta_path}")
    return out_tsv


def load_amrfinder_tsv(tsv_path: Path) -> pd.DataFrame:
    """Normalize AMRFinder TSV columns into a compact DataFrame."""

    df = pd.read_csv(tsv_path, sep="\t", comment="#", dtype=str).fillna("")
    lname = {c.lower().strip(): c for c in df.columns}

    def pick(*cands):
        for k in cands:
            if k in lname:
                return lname[k]
        return None

    contig_col = pick("contig id", "contig", "sequence")
    class_col = pick("class", "drug class", "drug_class")
    subclass_col = pick("subclass", "drug subclass", "drug_subclass")
    gene_col = pick("gene symbol", "gene", "symbol", "sequence name")
    aro_col = pick("aro accession", "aro", "aro_accession")

    if contig_col is None or class_col is None:
        raise ValueError(f"Missing required columns in {tsv_path}. Found: {list(df.columns)}")

    keep = [c for c in [contig_col, class_col, subclass_col, gene_col, aro_col] if c]
    out = df[keep].copy()

    new_cols = []
    for c in keep:
        lc = c.lower()
        if lc == contig_col.lower():
            new_cols.append("contig_id")
        elif lc == class_col.lower():
            new_cols.append("class")
        elif subclass_col and lc == subclass_col.lower():
            new_cols.append("subclass")
        elif gene_col and lc == gene_col.lower():
            new_cols.append("gene_symbol")
        elif aro_col and lc == aro_col.lower():
            new_cols.append("aro_id")
        else:
            new_cols.append(c)
    out.columns = new_cols
    return out


def clean_class(x: str) -> str:
    """Canonically format an AMR class string."""

    x = (x or "").strip()
    return x.replace("_", " ").replace("-", " ").title()


def all_contig_ids_from_fasta(fasta_path: Path) -> List[str]:
    ids: List[str] = []
    with open(fasta_path) as f:
        for line in f:
            if line.startswith(">"):
                ids.append(line[1:].strip().split()[0])
    return ids


def fasta_to_map(path: Path) -> dict[str, str]:
    m: dict[str, str] = {}
    with open(path, "r") as fh:
        header: str | None = None
        chunks: list[str] = []
        for line in fh:
            if line.startswith(">"):
                if header is not None:
                    m[header] = "".join(chunks)
                header = line[1:].strip().split()[0]
                chunks = []
            else:
                s = line.strip()
                if s:
                    chunks.append(s)
        if header is not None:
            m[header] = "".join(chunks)
    return m


def right_of_pipe(x: str) -> str:
    t = str(x)
    return t.split("|")[-1] if "|" in t else t


# --------------------------- Label preparation --------------------------

def build_amr_labels(
    tsv_paths: Sequence[Path],
    fasta_paths: Sequence[Path],
    output_dir: Path,
) -> AmrArtifacts:
    """Create contig-level AMR labels and save parquet/JSON artifacts."""

    hits: List[pd.DataFrame] = []
    for tsv in tsv_paths:
        sdf = load_amrfinder_tsv(tsv)
        sdf["source_file"] = Path(tsv).stem.replace(".amrfinder", "")
        hits.append(sdf)
    amr_hits = pd.concat(hits, ignore_index=True)
    amr_hits["class"] = amr_hits["class"].map(clean_class)

    all_classes = sorted([c for c in amr_hits["class"].unique() if c])
    class_to_idx = {c: i for i, c in enumerate(all_classes)}

    labels = (
        amr_hits[amr_hits["class"] != ""]
        .groupby(["source_file", "contig_id"])["class"]
        .apply(lambda xs: sorted(set(xs)))
        .reset_index()
    )

    def classes_to_multihot(classes: Sequence[str]) -> np.ndarray:
        vec = np.zeros(len(class_to_idx), dtype=np.int8)
        for c in classes:
            if c in class_to_idx:
                vec[class_to_idx[c]] = 1
        return vec

    labels["label_vector"] = labels["class"].apply(classes_to_multihot)
    labels["is_amr_positive"] = True

    all_contigs: list[tuple[str, str]] = []
    for fasta in fasta_paths:
        src = Path(fasta).stem
        ids = all_contig_ids_from_fasta(fasta)
        all_contigs.extend([(src, cid) for cid in ids])

    all_df = pd.DataFrame(all_contigs, columns=["source_file", "contig_id"])
    neg = all_df.merge(labels[["source_file", "contig_id", "is_amr_positive"]], how="left")
    neg = neg[neg["is_amr_positive"].isna()].drop(columns=["is_amr_positive"])
    neg["is_amr_positive"] = False
    neg["class"] = [[] for _ in range(len(neg))]
    neg["label_vector"] = [np.zeros(len(class_to_idx), dtype=np.int8) for _ in range(len(neg))]

    combined = pd.concat([labels, neg], ignore_index=True)
    combined["num_classes"] = combined["label_vector"].apply(lambda v: int(np.sum(v)))

    output_dir.mkdir(parents=True, exist_ok=True)
    labels_path = output_dir / "contig_amr_labels.parquet"
    classes_path = output_dir / "amr_class_list.json"
    combined.to_parquet(labels_path, index=False)
    with open(classes_path, "w") as jf:
        json.dump({"class_list": all_classes}, jf, indent=2)

    return AmrArtifacts(labels_path=labels_path, classes_path=classes_path)


def attach_sequences(df: pd.DataFrame, fasta_files: Sequence[Path]) -> pd.DataFrame:
    """Attach contig sequences from FASTA files to the label DataFrame."""

    seq_maps = [fasta_to_map(Path(p)) for p in fasta_files]
    from collections import ChainMap

    seq_map = dict(ChainMap(*seq_maps))
    df = df.copy()
    df["sequence"] = df["contig_id"].map(seq_map)

    missing = df["sequence"].isna()
    if missing.any():
        df.loc[missing, "sequence"] = df.loc[missing, "contig_id"].map(lambda cid: seq_map.get(right_of_pipe(cid)))
    return df


# --------------------------- Feature building ---------------------------

def hash_kmers_fast(seq: str, cfg: HashingConfig) -> np.ndarray:
    """Log-count hashed k-mer vectorization (adapted from the notebook)."""

    v = np.zeros(cfg.buckets, dtype=np.float32)
    s = str(seq).upper()
    if cfg.max_len is not None and len(s) > cfg.max_len:
        s = s[: cfg.max_len]
    n = len(s) - cfg.k + 1
    if n <= 0:
        return v
    i = 0
    while i < n:
        w = s[i : i + cfg.k]
        bad = False
        for c in w:
            oc = ord(c)
            if oc not in (65, 67, 71, 84):
                bad = True
                break
        if not bad:
            h = 0
            for c in w:
                h = (h * 131 + ord(c)) & 0xFFFFFFFF
            v[h % cfg.buckets] += 1.0
        i += cfg.stride
    return np.log1p(v)


def prepare_features(
    df: pd.DataFrame,
    class_list: Sequence[str],
    cfg: HashingConfig,
    threads: int = 1,
) -> tuple[np.ndarray, np.ndarray]:
    """Convert sequences and label vectors to NumPy arrays suitable for PyTorch."""

    seqs = df["sequence"].astype(str).tolist()
    workers = max(1, int(threads or 1))
    if workers > 1:
        with ThreadPoolExecutor(max_workers=workers) as ex:
            X_seq = np.stack(list(ex.map(lambda s: hash_kmers_fast(s, cfg), seqs)))
    else:
        X_seq = np.stack([hash_kmers_fast(s, cfg) for s in seqs])
    Y_amr = np.stack([
        np.array(v if isinstance(v, (list, tuple, np.ndarray)) else json.loads(v), dtype=np.float32)
        for v in df["label_vector"].values
    ])

    assert X_seq.shape[0] == Y_amr.shape[0], "Mismatched sequence/label rows"
    if Y_amr.shape[1] != len(class_list):
        raise ValueError("Label vectors do not match the AMR class list length")
    return X_seq, Y_amr


def train_val_test_split(
    df: pd.DataFrame,
    train_frac: float = 0.7,
    val_frac: float = 0.15,
    seed: int = 42,
) -> pd.DataFrame:
    """Add a ``split`` column with train/val/test assignments."""

    rng = np.random.default_rng(seed)
    df = df.copy()
    n = len(df)
    perm = rng.permutation(n)
    n_train = int(n * train_frac)
    n_val = int(n * val_frac)

    split = np.full(n, "test", dtype=object)
    split[perm[:n_train]] = "train"
    split[perm[n_train : n_train + n_val]] = "val"
    df["split"] = split
    return df


def load_taxonomy_edges(edges_tsv_path: Path, taxid2idx: dict[str, int]) -> np.ndarray | None:
    """Load taxonomy edges (child, parent) into an integer matrix."""

    p = Path(edges_tsv_path)
    if not p.exists():
        return None
    edges = pd.read_csv(p, sep="\t", header=None, names=["child", "parent"], dtype=str)
    edges = edges[edges["child"].isin(taxid2idx) & edges["parent"].isin(taxid2idx)]
    if edges.empty:
        return None
    return np.array([[taxid2idx[c], taxid2idx[p]] for c, p in edges[["child", "parent"]].values], dtype=np.int64)


