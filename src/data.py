"""Data utilities for the hyperbolic AMR detection pipeline.

This module lifts the preprocessing logic out of the exploratory notebook
(`amr_eda.ipynb`) into reusable functions:
- Download genomes (helper) and run AMRFinder on multi-FASTA genomes.
- Parse AMRFinder TSVs into canonical columns.
- Build positive/negative contig labels and AMR class lists.
- Attach sequences, hash them into fixed-length vectors, and split data.
"""

from __future__ import annotations

import json
import subprocess
import sys
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from pathlib import Path
from typing import List, Sequence
from urllib.parse import urlparse
from urllib.request import urlretrieve

import numpy as np
import pandas as pd


@dataclass
class AmrArtifacts:
    """Paths to data artifacts generated by the pipeline."""

    labels_path: Path
    classes_path: Path


@dataclass
class HashingConfig:
    k: int = 5
    buckets: int = 4096
    stride: int = 1
    max_len: int | None = None


# ----------------------------- I/O helpers -----------------------------

def download_genomes(urls: Sequence[str], output_dir: Path) -> List[Path]:
    """Download genomes from a list of URLs.

    This helper keeps the CLI self-contained; callers can point to any HTTP(S)
    endpoint hosting FASTA files. Files are saved under ``output_dir`` using the
    basename from the URL.
    """

    output_dir.mkdir(parents=True, exist_ok=True)
    paths: List[Path] = []
    for url in urls:
        target = output_dir / Path(urlparse(url).path).name
        target.parent.mkdir(parents=True, exist_ok=True)
        print(f"[download] {url} -> {target}")
        urlretrieve(url, target)
        paths.append(target)
    return paths


def run_amrfinder_on_fasta(
    fasta_path: Path,
    out_dir: Path,
    threads: int = 4,
    db: Path | None = None,
) -> Path:
    """Run AMRFinderPlus on a nucleotide FASTA and return the TSV path."""

    out_dir.mkdir(parents=True, exist_ok=True)
    fasta_path = Path(fasta_path)
    out_tsv = out_dir / f"{fasta_path.stem}.amrfinder.tsv"

    cmd = [
        "amrfinder",
        "--nucleotide",
        str(fasta_path),
        "--threads",
        str(threads),
        "--output",
        str(out_tsv),
        "--plus",
    ]
    if db is not None:
        cmd += ["--database", str(db)]

    print("[amrfinder]", " ".join(cmd))
    res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if res.returncode != 0:
        print("STDOUT:\n", res.stdout)
        print("STDERR:\n", res.stderr, file=sys.stderr)
        raise RuntimeError(f"AMRFinder failed on {fasta_path}")
    return out_tsv


def _pick_column(df: pd.DataFrame, *candidates: str) -> str | None:
    lname = {c.lower().strip(): c for c in df.columns}
    for cand in candidates:
        if cand in lname:
            return lname[cand]
    return None


def _contig_column(df: pd.DataFrame) -> str:
    col = _pick_column(df, "contig id", "contig", "sequence")
    if col is None:
        raise ValueError(f"Missing contig column in AMRFinder TSV. Found: {list(df.columns)}")
    return col


def load_amrfinder_tsv(tsv_path: Path) -> pd.DataFrame:
    """Normalize AMRFinder TSV columns into a compact DataFrame."""

    df = pd.read_csv(tsv_path, sep="\t", comment="#", dtype=str).fillna("")

    contig_col = _contig_column(df)
    class_col = _pick_column(df, "class", "drug class", "drug_class")
    subclass_col = _pick_column(df, "subclass", "drug subclass", "drug_subclass")
    gene_col = _pick_column(df, "gene symbol", "gene", "symbol", "sequence name")
    aro_col = _pick_column(df, "aro accession", "aro", "aro_accession")

    if contig_col is None or class_col is None:
        raise ValueError(f"Missing required columns in {tsv_path}. Found: {list(df.columns)}")

    keep = [c for c in [contig_col, class_col, subclass_col, gene_col, aro_col] if c]
    out = df[keep].copy()

    new_cols = []
    for c in keep:
        lc = c.lower()
        if lc == contig_col.lower():
            new_cols.append("contig_id")
        elif lc == class_col.lower():
            new_cols.append("class")
        elif subclass_col and lc == subclass_col.lower():
            new_cols.append("subclass")
        elif gene_col and lc == gene_col.lower():
            new_cols.append("gene_symbol")
        elif aro_col and lc == aro_col.lower():
            new_cols.append("aro_id")
        else:
            new_cols.append(c)
    out.columns = new_cols
    return out


def clean_class(x: str) -> str:
    """Canonically format an AMR class string."""

    x = (x or "").strip()
    return x.replace("_", " ").replace("-", " ").title()


def all_contig_ids_from_fasta(fasta_path: Path) -> List[str]:
    ids: List[str] = []
    with open(fasta_path) as f:
        for line in f:
            if line.startswith(">"):
                ids.append(line[1:].strip().split()[0])
    return ids


def fasta_to_map(path: Path) -> dict[str, str]:
    m: dict[str, str] = {}
    with open(path, "r") as fh:
        header: str | None = None
        chunks: list[str] = []
        for line in fh:
            if line.startswith(">"):
                if header is not None:
                    m[header] = "".join(chunks)
                header = line[1:].strip().split()[0]
                chunks = []
            else:
                s = line.strip()
                if s:
                    chunks.append(s)
        if header is not None:
            m[header] = "".join(chunks)
    return m


def right_of_pipe(x: str) -> str:
    t = str(x)
    return t.split("|")[-1] if "|" in t else t


def subsample_fasta_and_tsv(
    fasta_path: Path,
    tsv_path: Path,
    output_dir: Path,
    *,
    frac: float | None = None,
    n_contigs: int | None = None,
    max_contigs: int | None = None,
    seed: int = 42,
) -> tuple[Path, Path]:
    """Subsample contigs from a FASTA and filter the paired AMRFinder TSV.

    This avoids re-running AMRFinder by reusing existing TSV hits and FASTA
    sequences. Exactly one of ``frac`` (fraction of contigs to keep) or
    ``n_contigs`` (absolute count) must be provided; whichever is chosen sets
    the initial sample size relative to the number of contigs available. If
    ``max_contigs`` is provided, it acts as a hard per-FASTA cap applied after
    the fraction/count is computed (i.e., the final draw size is
    ``min(sample_size_from_frac_or_n, max_contigs)``).
    """

    if (frac is None) == (n_contigs is None):
        raise ValueError("Provide exactly one of 'frac' or 'n_contigs'")

    fasta_path = Path(fasta_path)
    tsv_path = Path(tsv_path)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    contig_ids = all_contig_ids_from_fasta(fasta_path)
    if not contig_ids:
        raise ValueError(f"No contigs found in {fasta_path}")

    rng = np.random.default_rng(seed)
    if n_contigs is not None:
        if n_contigs <= 0:
            raise ValueError("n_contigs must be positive")
        sample_size = min(n_contigs, len(contig_ids))
    else:
        if frac is None or not (0.0 < frac <= 1.0):
            raise ValueError("frac must be in (0, 1]")
        sample_size = max(1, int(np.floor(len(contig_ids) * frac)))
        sample_size = min(sample_size, len(contig_ids))

    if max_contigs is not None:
        if max_contigs <= 0:
            raise ValueError("max_contigs must be positive")
        sample_size = min(sample_size, max_contigs)

    chosen = set(rng.choice(contig_ids, size=sample_size, replace=False).tolist())

    fasta_out = output_dir / f"{fasta_path.stem}.subsampled.fasta"
    tsv_out = output_dir / f"{tsv_path.stem}.subsampled{tsv_path.suffix}"

    # Stream the FASTA to avoid holding all sequences in memory.
    with open(fasta_path) as fin, open(fasta_out, "w") as fout:
        write_block = False
        for line in fin:
            if line.startswith(">"):
                cid = line[1:].strip().split()[0]
                write_block = cid in chosen
            if write_block:
                fout.write(line)

    # Preserve AMRFinder comment lines (starting with '#') while filtering rows.
    comment_lines: list[str] = []
    with open(tsv_path) as tf:
        for line in tf:
            if line.startswith("#"):
                comment_lines.append(line.rstrip("\n"))
            else:
                break

    tsv_df = pd.read_csv(tsv_path, sep="\t", comment="#", dtype=str)
    contig_col = _contig_column(tsv_df)
    filtered = tsv_df[tsv_df[contig_col].astype(str).isin(chosen)]

    with open(tsv_out, "w") as out:
        for c in comment_lines:
            out.write(c + "\n")
        filtered.to_csv(out, sep="\t", index=False)

    return fasta_out, tsv_out


# --------------------------- Label preparation --------------------------

def build_amr_labels(
    tsv_paths: Sequence[Path],
    fasta_paths: Sequence[Path],
    output_dir: Path,
) -> AmrArtifacts:
    """Create contig-level AMR labels and save parquet/JSON artifacts."""

    hits: List[pd.DataFrame] = []
    for tsv in tsv_paths:
        sdf = load_amrfinder_tsv(tsv)
        sdf["source_file"] = Path(tsv).stem.replace(".amrfinder", "")
        hits.append(sdf)
    amr_hits = pd.concat(hits, ignore_index=True)
    amr_hits["class"] = amr_hits["class"].map(clean_class)

    all_classes = sorted([c for c in amr_hits["class"].unique() if c])
    class_to_idx = {c: i for i, c in enumerate(all_classes)}

    labels = (
        amr_hits[amr_hits["class"] != ""]
        .groupby(["source_file", "contig_id"])["class"]
        .apply(lambda xs: sorted(set(xs)))
        .reset_index()
    )

    def classes_to_multihot(classes: Sequence[str]) -> np.ndarray:
        vec = np.zeros(len(class_to_idx), dtype=np.int8)
        for c in classes:
            if c in class_to_idx:
                vec[class_to_idx[c]] = 1
        return vec

    labels["label_vector"] = labels["class"].apply(classes_to_multihot)
    labels["is_amr_positive"] = True

    all_contigs: list[tuple[str, str]] = []
    for fasta in fasta_paths:
        src = Path(fasta).stem
        ids = all_contig_ids_from_fasta(fasta)
        all_contigs.extend([(src, cid) for cid in ids])

    all_df = pd.DataFrame(all_contigs, columns=["source_file", "contig_id"])
    neg = all_df.merge(labels[["source_file", "contig_id", "is_amr_positive"]], how="left")
    neg = neg[neg["is_amr_positive"].isna()].drop(columns=["is_amr_positive"])
    neg["is_amr_positive"] = False
    neg["class"] = [[] for _ in range(len(neg))]
    neg["label_vector"] = [np.zeros(len(class_to_idx), dtype=np.int8) for _ in range(len(neg))]

    combined = pd.concat([labels, neg], ignore_index=True)
    combined["num_classes"] = combined["label_vector"].apply(lambda v: int(np.sum(v)))

    output_dir.mkdir(parents=True, exist_ok=True)
    labels_path = output_dir / "contig_amr_labels.parquet"
    classes_path = output_dir / "amr_class_list.json"
    combined.to_parquet(labels_path, index=False)
    with open(classes_path, "w") as jf:
        json.dump({"class_list": all_classes}, jf, indent=2)

    return AmrArtifacts(labels_path=labels_path, classes_path=classes_path)


def attach_taxonomy(
    df: pd.DataFrame,
    taxonomy_map_path: Path,
    lineage_path: Path | None = None,
) -> pd.DataFrame:
    """Attach taxonomic identifiers/lineages to the label DataFrame.

    ``taxonomy_map_path`` should be a TSV that includes a ``taxid`` column and
    at least one join key among ``contig_id`` or ``source_file``. Additional
    lineage columns present in the mapping file are preserved. If
    ``lineage_path`` is provided, it should be a TSV with ``taxid`` plus lineage
    ranks (e.g., ``domain``, ``phylum``, ...); any missing lineage columns will
    be joined from that table.
    """

    df = df.copy()
    taxonomy_map = pd.read_csv(taxonomy_map_path, sep="\t")
    if "taxid" not in taxonomy_map.columns:
        raise ValueError("taxonomy map must include a 'taxid' column")

    join_keys = [c for c in ("source_file", "contig_id") if c in taxonomy_map.columns]
    if not join_keys:
        raise ValueError("taxonomy map must include 'source_file' or 'contig_id' for joining")

    merged = df.merge(taxonomy_map, on=join_keys, how="left")

    if lineage_path is not None:
        lineage = pd.read_csv(lineage_path, sep="\t")
        if "taxid" not in lineage.columns:
            raise ValueError("taxonomy lineage file must include a 'taxid' column")

        lineage_cols = [c for c in lineage.columns if c != "taxid"]
        # Only bring in lineage columns that are missing from the merged DataFrame.
        missing_cols = [c for c in lineage_cols if c not in merged.columns]
        if missing_cols:
            merged = merged.merge(lineage[["taxid", *missing_cols]], on="taxid", how="left")

    return merged


def attach_sequences(df: pd.DataFrame, fasta_files: Sequence[Path]) -> pd.DataFrame:
    """Attach contig sequences from FASTA files to the label DataFrame."""

    seq_maps = [fasta_to_map(Path(p)) for p in fasta_files]
    from collections import ChainMap

    seq_map = dict(ChainMap(*seq_maps))
    df = df.copy()
    df["sequence"] = df["contig_id"].map(seq_map)

    missing = df["sequence"].isna()
    if missing.any():
        df.loc[missing, "sequence"] = df.loc[missing, "contig_id"].map(lambda cid: seq_map.get(right_of_pipe(cid)))
    return df


# --------------------------- Dataset diagnostics ---------------------------

def amr_balance_by_source(
    df: pd.DataFrame, class_list: Sequence[str] | None = None
) -> pd.DataFrame:
    """Summarize AMR label balance per ``source_file``.

    The returned DataFrame includes overall counts of AMR-positive/negative
    contigs and per-class totals for each source. ``class_list`` (if provided)
    names the columns corresponding to ``label_vector`` entries; otherwise
    class names are inferred from ``df['class']`` when available or fall back
    to generic ``class_#`` labels.
    """

    if "source_file" not in df.columns:
        raise ValueError("contig_amr_labels must include a 'source_file' column")
    if "label_vector" not in df.columns:
        raise ValueError("contig_amr_labels must include a 'label_vector' column")

    def _to_array(v):
        if isinstance(v, (list, tuple, np.ndarray)):
            return np.array(v, dtype=np.int64)
        try:
            return np.array(json.loads(v), dtype=np.int64)
        except TypeError as exc:  # pragma: no cover - defensive
            raise ValueError("label_vector entries must be list-like") from exc

    label_matrix = np.stack([_to_array(v) for v in df["label_vector"].values])

    if class_list is None:
        inferred: list[str] = []
        if "class" in df.columns:
            for row in df["class"]:
                if isinstance(row, (list, tuple, set)):
                    inferred.extend(row)
                elif isinstance(row, str) and row.startswith("["):
                    try:
                        inferred.extend(json.loads(row))
                    except json.JSONDecodeError:
                        pass
        inferred = sorted(set(inferred))
        classes = inferred if len(inferred) == label_matrix.shape[1] else [
            f"class_{i}" for i in range(label_matrix.shape[1])
        ]
    else:
        classes = list(class_list)
        if len(classes) != label_matrix.shape[1]:
            raise ValueError(
                "class_list length does not match the width of label_vector entries"
            )

    is_positive = (
        df["is_amr_positive"].astype(bool).to_numpy()
        if "is_amr_positive" in df.columns
        else label_matrix.sum(axis=1) > 0
    )

    class_df = pd.DataFrame(label_matrix, columns=classes)
    grouped = pd.concat(
        [df[["source_file"]].reset_index(drop=True), pd.Series(is_positive, name="is_amr_positive"), class_df],
        axis=1,
    ).groupby("source_file")

    summary = grouped["is_amr_positive"].agg(num_contigs="count", num_positive="sum")
    class_counts = grouped[classes].sum()

    out = summary.join(class_counts)
    out["num_negative"] = out["num_contigs"] - out["num_positive"]
    out["positive_frac"] = out["num_positive"] / out["num_contigs"]

    ordered_cols = ["source_file", "num_contigs", "num_positive", "num_negative", "positive_frac", *classes]
    return out.reset_index()[ordered_cols]


# --------------------------- Feature building ---------------------------

def hash_kmers_fast(seq: str, cfg: HashingConfig) -> np.ndarray:
    """Log-count hashed k-mer vectorization (adapted from the notebook)."""

    v = np.zeros(cfg.buckets, dtype=np.float32)
    s = str(seq).upper()
    if cfg.max_len is not None and len(s) > cfg.max_len:
        s = s[: cfg.max_len]
    n = len(s) - cfg.k + 1
    if n <= 0:
        return v
    i = 0
    while i < n:
        w = s[i : i + cfg.k]
        bad = False
        for c in w:
            oc = ord(c)
            if oc not in (65, 67, 71, 84):
                bad = True
                break
        if not bad:
            h = 0
            for c in w:
                h = (h * 131 + ord(c)) & 0xFFFFFFFF
            v[h % cfg.buckets] += 1.0
        i += cfg.stride
    return np.log1p(v)


def prepare_features(
    df: pd.DataFrame,
    class_list: Sequence[str],
    cfg: HashingConfig,
    threads: int = 1,
) -> tuple[np.ndarray, np.ndarray]:
    """Convert sequences and label vectors to NumPy arrays suitable for PyTorch."""

    seqs = df["sequence"].astype(str).tolist()
    workers = max(1, int(threads or 1))
    if workers > 1:
        with ThreadPoolExecutor(max_workers=workers) as ex:
            X_seq = np.stack(list(ex.map(lambda s: hash_kmers_fast(s, cfg), seqs)))
    else:
        X_seq = np.stack([hash_kmers_fast(s, cfg) for s in seqs])
    Y_amr = np.stack([
        np.array(v if isinstance(v, (list, tuple, np.ndarray)) else json.loads(v), dtype=np.float32)
        for v in df["label_vector"].values
    ])

    assert X_seq.shape[0] == Y_amr.shape[0], "Mismatched sequence/label rows"
    if Y_amr.shape[1] != len(class_list):
        raise ValueError("Label vectors do not match the AMR class list length")
    return X_seq, Y_amr


def train_val_test_split(
    df: pd.DataFrame,
    train_frac: float = 0.7,
    val_frac: float = 0.15,
    seed: int = 42,
) -> pd.DataFrame:
    """Add a ``split`` column with train/val/test assignments."""

    rng = np.random.default_rng(seed)
    df = df.copy()
    n = len(df)
    perm = rng.permutation(n)
    n_train = int(n * train_frac)
    n_val = int(n * val_frac)

    split = np.full(n, "test", dtype=object)
    split[perm[:n_train]] = "train"
    split[perm[n_train : n_train + n_val]] = "val"
    df["split"] = split
    return df


def encode_taxonomy_lineage(df: pd.DataFrame, columns: Sequence[str]) -> tuple[np.ndarray | None, dict[str, int]]:
    """Map taxonomy lineage columns to contiguous integer indices.

    Returns an ``(N, L)`` matrix of integer indices (padding=-1) and a mapping
    from original taxon IDs to indices. The index ``0`` is reserved for padding;
    real taxa start at 1.
    """

    if not columns:
        return None, {}

    missing = [c for c in columns if c not in df.columns]
    if missing:
        raise ValueError(f"Missing taxonomy columns: {missing}")

    uniq_tax: list[str] = []
    for col in columns:
        vals = df[col].astype(str).fillna("")
        uniq_tax.extend([v for v in vals if v and v != "-1" and v.lower() != "nan"])
    uniq_tax_sorted = sorted(set(uniq_tax))
    if not uniq_tax_sorted:
        return None, {}

    taxid2idx = {t: i + 1 for i, t in enumerate(uniq_tax_sorted)}  # 0 = padding
    taxonomy = np.full((len(df), len(columns)), -1, dtype=np.int64)

    for j, col in enumerate(columns):
        vals = df[col].astype(str).fillna("-1")
        mask = (vals != "-1") & (vals != "") & (vals.str.lower() != "nan")
        if mask.any():
            taxonomy[mask.to_numpy(), j] = [taxid2idx[v] for v in vals[mask]]

    return taxonomy, taxid2idx


def load_taxonomy_edges(edges_tsv_path: Path, taxid2idx: dict[str, int]) -> np.ndarray | None:
    """Load taxonomy edges (child, parent) into an integer matrix."""

    p = Path(edges_tsv_path)
    if not p.exists():
        return None
    edges = pd.read_csv(p, sep="\t", header=None, names=["child", "parent"], dtype=str)
    edges = edges[edges["child"].isin(taxid2idx) & edges["parent"].isin(taxid2idx)]
    if edges.empty:
        return None
    return np.array([[taxid2idx[c], taxid2idx[p]] for c, p in edges[["child", "parent"]].values], dtype=np.int64)


